{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Installation",
   "id": "320806866575b715"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-19T11:51:10.673740Z",
     "start_time": "2025-07-19T11:51:10.238030Z"
    }
   },
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T11:51:35.892910Z",
     "start_time": "2025-07-19T11:51:35.350157Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install python-dotenv huggingface_hub datasets wandb rouge-score nltk scikit-learn",
   "id": "29a3c7a931539016",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;31merror\u001B[0m: \u001B[1mexternally-managed-environment\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m This environment is externally managed\r\n",
      "\u001B[31m╰─>\u001B[0m To install Python packages system-wide, try 'pacman -S\r\n",
      "\u001B[31m   \u001B[0m python-xyz', where xyz is the package you are trying to\r\n",
      "\u001B[31m   \u001B[0m install.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Arch-packaged Python package,\r\n",
      "\u001B[31m   \u001B[0m create a virtual environment using 'python -m venv path/to/venv'.\r\n",
      "\u001B[31m   \u001B[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Arch packaged Python application,\r\n",
      "\u001B[31m   \u001B[0m it may be easiest to use 'pipx install xyz', which will manage a\r\n",
      "\u001B[31m   \u001B[0m virtual environment for you. Make sure you have python-pipx\r\n",
      "\u001B[31m   \u001B[0m installed via pacman.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001B[1;36mhint\u001B[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Unsloth",
   "id": "59f1eb5bfefd47c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:21.318826Z",
     "start_time": "2025-07-19T14:24:56.002992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ],
   "id": "37f341229268ddd7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.623 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
   "id": "88075385299f7223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:31.587611Z",
     "start_time": "2025-07-19T14:25:29.133993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ],
   "id": "8338a0e7da183a51",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Download Dataset",
   "id": "1f0028fc5d5e3a74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:47.254942Z",
     "start_time": "2025-07-19T14:25:32.965746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "file_id = '1zPCJZR69yBbBYp6oFrDrpbuJvvMpVB72'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output='cleaned_headline_dataset.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "id": "b47ea5e1d05a0122",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;31merror\u001B[0m: \u001B[1mexternally-managed-environment\u001B[0m\r\n",
      "\r\n",
      "\u001B[31m×\u001B[0m This environment is externally managed\r\n",
      "\u001B[31m╰─>\u001B[0m To install Python packages system-wide, try 'pacman -S\r\n",
      "\u001B[31m   \u001B[0m python-xyz', where xyz is the package you are trying to\r\n",
      "\u001B[31m   \u001B[0m install.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Arch-packaged Python package,\r\n",
      "\u001B[31m   \u001B[0m create a virtual environment using 'python -m venv path/to/venv'.\r\n",
      "\u001B[31m   \u001B[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\r\n",
      "\u001B[31m   \u001B[0m \r\n",
      "\u001B[31m   \u001B[0m If you wish to install a non-Arch packaged Python application,\r\n",
      "\u001B[31m   \u001B[0m it may be easiest to use 'pipx install xyz', which will manage a\r\n",
      "\u001B[31m   \u001B[0m virtual environment for you. Make sure you have python-pipx\r\n",
      "\u001B[31m   \u001B[0m installed via pacman.\r\n",
      "\r\n",
      "\u001B[1;35mnote\u001B[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001B[1;36mhint\u001B[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zPCJZR69yBbBYp6oFrDrpbuJvvMpVB72\n",
      "To: /home/siam/Personal/news2headline/src/cleaned_headline_dataset.csv\n",
      "100%|██████████| 100M/100M [00:03<00:00, 26.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cleaned_headline_dataset.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I'm great thanks!<|eot_id|>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
   ],
   "id": "d12dd2785e1b49ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:47.324612Z",
     "start_time": "2025-07-19T14:25:47.321252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "# ─── 1. TOKENIZER SETUP ─────────────────────────────────────────────────────────\n",
    "\n",
    "# initialize Unsloth's chat tokenizer for llama‑3.1\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Apply your chat template to create conversation format.\"\"\"\n",
    "    conversations = []\n",
    "    for content, headline in zip(examples[\"content\"], examples[\"headline\"]):\n",
    "        # Create a conversation with user question and assistant answer\n",
    "        conversation = [\n",
    "            {\"role\": \"user\", \"content\": content},\n",
    "            {\"role\": \"assistant\", \"content\": headline}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    # Apply chat template to each conversation\n",
    "    texts = []\n",
    "    for conversation in conversations:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}"
   ],
   "id": "392c10be6c091a5f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:52.538976Z",
     "start_time": "2025-07-19T14:25:50.447672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# ─── 2. LOAD CSV & SPLIT ────────────────────────────────────────────────────────\n",
    "\n",
    "# load the CSV from local disk\n",
    "raw = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"full\": \"cleaned_headline_dataset.csv\"},\n",
    "    split=\"full\"\n",
    ")\n",
    "\n",
    "# first split off test (10% of full), then split the remaining 90% into train (80%) & validation (10%)\n",
    "split1 = raw.train_test_split(test_size=0.10, seed=42)\n",
    "split2 = split1[\"train\"].train_test_split(test_size=0.11, seed=42)\n",
    "# note: 0.11 of 90% ≈ 10% of total\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": split2[\"train\"],\n",
    "    \"validation\": split2[\"test\"],\n",
    "    \"test\": split1[\"test\"],\n",
    "})\n"
   ],
   "id": "ff2d34a3e1bb0660",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "179d4316c94946708bd29e5674579d5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:56.358260Z",
     "start_time": "2025-07-19T14:25:54.261969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ─── 3. APPLY FORMATTING TO DATASETS ───────────────────────────────────────────\n",
    "\n",
    "# Apply the formatting function to all datasets\n",
    "datasets = datasets.map(formatting_prompts_func, batched=True)"
   ],
   "id": "68155515e7fdc35e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/41681 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d74366753e34e0c9fec250c62eec77a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5152 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a103e57a72445d7b208f22b21183be5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5204 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11ffd2283ef54082b7f4b6af7e6fbea1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:32.879715Z",
     "start_time": "2025-07-19T14:34:29.265286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ─── 4. WANDB AUTHENTICATION & SETUP (CORRECTED) ───────────────────────────────\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from glob import glob\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up wandb environment variables to avoid warnings\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"TinyLlama_Headline_Generation.ipynb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"news2headline-tinyllama\"\n",
    "\n",
    "# Get wandb token and set as environment variable (PREFERRED METHOD)\n",
    "wandb_token = os.getenv(\"WANDB_KEY\")\n",
    "if wandb_token:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
    "    print(\"✅ Wandb API key set via environment variable\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: WANDB_KEY not found in .env file\")\n",
    "    print(\"   Add this line to your .env file: WANDB_KEY=your_api_key_here\")\n",
    "    print(\"   Get your API key from: https://wandb.ai/authorize\")\n",
    "\n",
    "# Initialize wandb (this will automatically use WANDB_API_KEY if set)\n",
    "wandb.init(\n",
    "    project=\"news2headline-tinyllama\",\n",
    "    name=\"tinyllama-1.1b-headline-generation\",\n",
    "    notes=\"Fine-tuning TinyLlama 1.1B for news headline generation using Unsloth\",\n",
    "    tags=[\"tinyllama\", \"headline-generation\", \"unsloth\", \"lora\"],\n",
    "    config={\n",
    "        \"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"lora_r\": 16,\n",
    "        \"lora_alpha\": 16,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"task\": \"headline_generation\",\n",
    "        \"dataset_size\": len(datasets[\"train\"]),\n",
    "        \"validation_size\": len(datasets[\"validation\"]),\n",
    "        \"test_size\": len(datasets[\"test\"])\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"🚀 Wandb run initialized: {wandb.run.name}\")\n",
    "print(f\"📊 Dashboard: {wandb.run.url}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "output_dir = \"outputs\"\n",
    "checkpoint_dirs = glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Sort by checkpoint number and get the latest one\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = checkpoint_dirs[-1]\n",
    "    print(f\"Found checkpoint: {resume_from_checkpoint}\")\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from beginning...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = datasets[\"train\"],\n",
    "    eval_dataset = datasets[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        # max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "        report_to = \"wandb\", # Enable wandb logging\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 500,  # Save checkpoint every 500 steps\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 500,  # Evaluate every 500 steps\n",
    "        logging_strategy = \"steps\",\n",
    "        load_best_model_at_end = True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "    ),\n",
    ")"
   ],
   "id": "8fdd81f19f0729e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wandb API key set via environment variable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tinyllama-1.1b-headline-generation</strong> at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/7o281ukl' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/7o281ukl</a><br> View project at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250719_203149-7o281ukl/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/siam/Personal/news2headline/src/wandb/run-20250719_203429-lk5p5ypv</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv' target=\"_blank\">tinyllama-1.1b-headline-generation</a></strong> to <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Wandb run initialized: tinyllama-1.1b-headline-generation\n",
      "📊 Dashboard: https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv\n",
      "No checkpoint found. Starting training from beginning...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:37.552526Z",
     "start_time": "2025-07-19T14:34:35.269930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ─── 5. APPLY TRAIN ON RESPONSES ONLY ──────────────────────────────────────────\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ],
   "id": "c4eff449bb196e7e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/41681 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d20953faa9c14526b36010b1c2a4e60a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/5152 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0bb36ae5d2f543cf9434448fa47fc0ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:38.506474Z",
     "start_time": "2025-07-19T14:34:38.500988Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])",
   "id": "8552e81babe2d058",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVeet, in celebration of its 20th anniversary in Bangladesh, is launching a special television series titled “Agiye Jao Attobisshashey” with Deepto TV, to honor the strength, resilience, and courage of the women in Bangladesh. The show will premiere on Deepto TV on November 1, 2024, and will air every Friday and Saturday at 9:20 pm. The program will be re-telecasted on Saturday and Sunday at 4.50pm. The popular Bangladeshi actress and model, Zakia Bari Mamo, will host the show. Veet’s anniversary marks two decades of helping women feel beautiful and confident in Bangladesh. The title “Agiye Jao Attobisshashey”, which means \"Move Forward with Confidence,\" reflects Veet’s commitment to inspiring women and giving them a platform to share their powerful life stories. The series will feature ten inspiring women from different backgrounds and professions who have made a turn-around in her life in spite of lot of obstacles. These women have overcome challenges, broken stereotypes, and dedicated themselves and eventually came where they are today. From business leaders to teachers, athletes to advocates, these women reflect the values of strength and determination that Veet has always supported in 20 years of its journey in Bangladesh. Bd-pratidin English/Lutful Hoque<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nVeet celebrates 20 Years of journey with \"Agiye Jao Attobisshashey” campaign<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:40.502285Z",
     "start_time": "2025-07-19T14:34:40.497178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ],
   "id": "3610a95286d01405",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                             Veet celebrates 20 Years of journey with \"Agiye Jao Attobisshashey” campaign<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-19T14:34:43.212508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ─── 6. START TRAINING WITH CHECKPOINT RESUMING ───────────────────────────────\n",
    "\n",
    "# Start training - will automatically resume from checkpoint if available\n",
    "# According to Unsloth docs, use resume_from_checkpoint=True for automatic detection\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True if resume_from_checkpoint else None)\n",
    "\n",
    "# Log final metrics to wandb\n",
    "wandb.log({\n",
    "    \"final_train_loss\": trainer_stats.training_loss,\n",
    "    \"total_steps\": trainer_stats.global_step,\n",
    "})\n",
    "\n",
    "print(f\"Training completed! Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Total training steps: {trainer_stats.global_step}\")"
   ],
   "id": "fdf74f22f97c503f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 41,681 | Num Epochs = 1 | Total steps = 5,211\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='5211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 114/5211 03:05 < 2:20:20, 0.61 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Evaluation on Test Set",
   "id": "evaluation_section"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ─── 7. COMPREHENSIVE MODEL EVALUATION ─────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Starting comprehensive evaluation on test set...\")\n",
    "print(f\"Test set size: {len(datasets['test'])}\")\n",
    "\n",
    "# Evaluate on validation set first (faster)\n",
    "print(\"\\n=== Validation Set Evaluation ===\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=datasets[\"validation\"])\n",
    "print(f\"Validation Loss: {val_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Validation Perplexity: {np.exp(val_metrics['eval_loss']):.4f}\")\n",
    "\n",
    "# Log validation metrics to wandb\n",
    "wandb.log({\n",
    "    \"val_loss\": val_metrics['eval_loss'],\n",
    "    \"val_perplexity\": np.exp(val_metrics['eval_loss'])\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "evaluation_metrics"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ─── 8. QUALITATIVE EVALUATION WITH GENERATION METRICS ────────────────────────\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_headline(content, max_length=50):\n",
    "    \"\"\"Generate headline for given content\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test on a subset of test data for detailed metrics (first 100 samples for speed)\n",
    "test_subset = datasets[\"test\"].select(range(min(100, len(datasets[\"test\"]))))\n",
    "print(f\"\\n=== Generating headlines for {len(test_subset)} test samples ===\")\n",
    "\n",
    "generated_headlines = []\n",
    "reference_headlines = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_subset)):\n",
    "    try:\n",
    "        generated = generate_headline(example[\"content\"])\n",
    "        generated_headlines.append(generated)\n",
    "        reference_headlines.append(example[\"headline\"])\n",
    "        \n",
    "        # Show first few examples\n",
    "        if i < 3:\n",
    "            print(f\"\\n--- Example {i+1} ---\")\n",
    "            print(f\"Content: {example['content'][:200]}...\")\n",
    "            print(f\"Reference: {example['headline']}\")\n",
    "            print(f\"Generated: {generated}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating headline for example {i}: {e}\")\n",
    "        generated_headlines.append(\"\")\n",
    "        reference_headlines.append(example[\"headline\"])\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len([h for h in generated_headlines if h])} headlines\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "generation_evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ─── 9. CALCULATE COMPREHENSIVE METRICS ────────────────────────────────────────\n",
    "\n",
    "def calculate_rouge_scores(generated, references):\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for gen, ref in zip(generated, references):\n",
    "        if gen:  # Only calculate if generation is not empty\n",
    "            scores = scorer.score(ref, gen)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0,\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0,\n",
    "        'rougeL': np.mean(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0\n",
    "    }\n",
    "\n",
    "def calculate_bleu_scores(generated, references):\n",
    "    \"\"\"Calculate BLEU scores\"\"\"\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated, references):\n",
    "        if gen:  # Only calculate if generation is not empty\n",
    "            ref_tokens = [ref.split()]\n",
    "            gen_tokens = gen.split()\n",
    "            try:\n",
    "                bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothing)\n",
    "                bleu_scores.append(bleu)\n",
    "            except:\n",
    "                bleu_scores.append(0.0)\n",
    "    \n",
    "    return np.mean(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "def calculate_length_metrics(generated, references):\n",
    "    \"\"\"Calculate length-based metrics\"\"\"\n",
    "    gen_lengths = [len(h.split()) for h in generated if h]\n",
    "    ref_lengths = [len(h.split()) for h in references]\n",
    "    \n",
    "    return {\n",
    "        'avg_generated_length': np.mean(gen_lengths) if gen_lengths else 0,\n",
    "        'avg_reference_length': np.mean(ref_lengths),\n",
    "        'length_ratio': np.mean(gen_lengths) / np.mean(ref_lengths) if gen_lengths and ref_lengths else 0\n",
    "    }\n",
    "\n",
    "# Calculate all metrics\n",
    "print(\"\\n=== Calculating Evaluation Metrics ===\")\n",
    "\n",
    "# ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores(generated_headlines, reference_headlines)\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "# BLEU score\n",
    "bleu_score = calculate_bleu_scores(generated_headlines, reference_headlines)\n",
    "print(f\"BLEU: {bleu_score:.4f}\")\n",
    "\n",
    "# Length metrics\n",
    "length_metrics = calculate_length_metrics(generated_headlines, reference_headlines)\n",
    "print(f\"Avg Generated Length: {length_metrics['avg_generated_length']:.2f} words\")\n",
    "print(f\"Avg Reference Length: {length_metrics['avg_reference_length']:.2f} words\")\n",
    "print(f\"Length Ratio: {length_metrics['length_ratio']:.2f}\")\n",
    "\n",
    "# Success rate\n",
    "success_rate = len([h for h in generated_headlines if h]) / len(generated_headlines)\n",
    "print(f\"Generation Success Rate: {success_rate:.2%}\")\n",
    "\n",
    "# Compile final metrics\n",
    "final_metrics = {\n",
    "    \"test_rouge1\": rouge_scores['rouge1'],\n",
    "    \"test_rouge2\": rouge_scores['rouge2'],\n",
    "    \"test_rougeL\": rouge_scores['rougeL'],\n",
    "    \"test_bleu\": bleu_score,\n",
    "    \"test_success_rate\": success_rate,\n",
    "    \"test_avg_gen_length\": length_metrics['avg_generated_length'],\n",
    "    \"test_avg_ref_length\": length_metrics['avg_reference_length'],\n",
    "    \"test_length_ratio\": length_metrics['length_ratio']\n",
    "}\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log(final_metrics)\n",
    "\n",
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Performance on News Headline Generation:\")\n",
    "print(f\"• ROUGE-1 (Unigram Overlap): {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"• ROUGE-2 (Bigram Overlap): {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"• ROUGE-L (Longest Common Subsequence): {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"• BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"• Generation Success Rate: {success_rate:.2%}\")\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\nEvaluation completed! Check your wandb dashboard for detailed metrics.\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "comprehensive_metrics"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
