{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320806866575b715",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T11:51:10.673740Z",
     "start_time": "2025-07-19T11:51:10.238030Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29a3c7a931539016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T11:51:35.892910Z",
     "start_time": "2025-07-19T11:51:35.350157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try 'pacman -S\r\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\r\n",
      "\u001b[31m   \u001b[0m installed via pacman.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv huggingface_hub datasets wandb rouge-score nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f1eb5bfefd47c",
   "metadata": {},
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37f341229268ddd7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:21.318826Z",
     "start_time": "2025-07-19T14:24:56.002992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.7.5: Fast Llama patching. Transformers: 4.53.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4060 Laptop GPU. Num GPUs = 1. Max memory: 7.623 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.9. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88075385299f7223",
   "metadata": {},
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8338a0e7da183a51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:31.587611Z",
     "start_time": "2025-07-19T14:25:29.133993Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.7.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0028fc5d5e3a74",
   "metadata": {},
   "source": [
    "#### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b47ea5e1d05a0122",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:47.254942Z",
     "start_time": "2025-07-19T14:25:32.965746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\r\n",
      "\r\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\r\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try 'pacman -S\r\n",
      "\u001b[31m   \u001b[0m python-xyz', where xyz is the package you are trying to\r\n",
      "\u001b[31m   \u001b[0m install.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch-packaged Python package,\r\n",
      "\u001b[31m   \u001b[0m create a virtual environment using 'python -m venv path/to/venv'.\r\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip.\r\n",
      "\u001b[31m   \u001b[0m \r\n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Arch packaged Python application,\r\n",
      "\u001b[31m   \u001b[0m it may be easiest to use 'pipx install xyz', which will manage a\r\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have python-pipx\r\n",
      "\u001b[31m   \u001b[0m installed via pacman.\r\n",
      "\r\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\r\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zPCJZR69yBbBYp6oFrDrpbuJvvMpVB72\n",
      "To: /home/siam/Personal/news2headline/src/cleaned_headline_dataset.csv\n",
      "100%|██████████| 100M/100M [00:03<00:00, 26.1MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cleaned_headline_dataset.csv'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install gdown\n",
    "import gdown\n",
    "file_id = '1zPCJZR69yBbBYp6oFrDrpbuJvvMpVB72'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output='cleaned_headline_dataset.csv'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12dd2785e1b49ad",
   "metadata": {},
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `Llama-3.1` format for conversation style finetunes. We use [Maxime Labonne's FineTome-100k](https://huggingface.co/datasets/mlabonne/FineTome-100k) dataset in ShareGPT style. But we convert it to HuggingFace's normal multiturn format `(\"role\", \"content\")` instead of `(\"from\", \"value\")`/ Llama-3 renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "Hello!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "Hey there! How are you?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "I'm great thanks!<|eot_id|>\n",
    "```\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, phi3, llama3` and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "392c10be6c091a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:47.324612Z",
     "start_time": "2025-07-19T14:25:47.321252Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import get_chat_template\n",
    "\n",
    "# ─── 1. TOKENIZER SETUP ─────────────────────────────────────────────────────────\n",
    "\n",
    "# initialize Unsloth's chat tokenizer for llama‑3.1\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template=\"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"Apply your chat template to create conversation format.\"\"\"\n",
    "    conversations = []\n",
    "    for content, headline in zip(examples[\"content\"], examples[\"headline\"]):\n",
    "        # Create a conversation with user question and assistant answer\n",
    "        conversation = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert news headline writer. Create concise, engaging headlines that capture the main story. Keep headlines between 5-12 words.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Write a headline for this news article:\\n\\n{content}\"},\n",
    "            {\"role\": \"assistant\", \"content\": headline}\n",
    "        ]\n",
    "        conversations.append(conversation)\n",
    "\n",
    "    # Apply chat template to each conversation\n",
    "    texts = []\n",
    "    for conversation in conversations:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "\n",
    "    return {\"text\": texts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff2d34a3e1bb0660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:52.538976Z",
     "start_time": "2025-07-19T14:25:50.447672Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179d4316c94946708bd29e5674579d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating full split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# ─── 2. LOAD CSV & SPLIT ────────────────────────────────────────────────────────\n",
    "\n",
    "# load the CSV from local disk\n",
    "raw = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\"full\": \"cleaned_headline_dataset.csv\"},\n",
    "    split=\"full\"\n",
    ")\n",
    "\n",
    "# first split off test (10% of full), then split the remaining 90% into train (80%) & validation (10%)\n",
    "split1 = raw.train_test_split(test_size=0.10, seed=42)\n",
    "split2 = split1[\"train\"].train_test_split(test_size=0.11, seed=42)\n",
    "# note: 0.11 of 90% ≈ 10% of total\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    \"train\": split2[\"train\"],\n",
    "    \"validation\": split2[\"test\"],\n",
    "    \"test\": split1[\"test\"],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68155515e7fdc35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:25:56.358260Z",
     "start_time": "2025-07-19T14:25:54.261969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d74366753e34e0c9fec250c62eec77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a103e57a72445d7b208f22b21183be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ffd2283ef54082b7f4b6af7e6fbea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 3. APPLY FORMATTING TO DATASETS ───────────────────────────────────────────\n",
    "\n",
    "# Apply the formatting function to all datasets\n",
    "datasets = datasets.map(formatting_prompts_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fdd81f19f0729e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:32.879715Z",
     "start_time": "2025-07-19T14:34:29.265286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wandb API key set via environment variable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tinyllama-1.1b-headline-generation</strong> at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/7o281ukl' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/7o281ukl</a><br> View project at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250719_203149-7o281ukl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/siam/Personal/news2headline/src/wandb/run-20250719_203429-lk5p5ypv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv' target=\"_blank\">tinyllama-1.1b-headline-generation</a></strong> to <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Wandb run initialized: tinyllama-1.1b-headline-generation\n",
      "📊 Dashboard: https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv\n",
      "No checkpoint found. Starting training from beginning...\n"
     ]
    }
   ],
   "source": [
    "# ─── 4. WANDB AUTHENTICATION & SETUP (CORRECTED) ───────────────────────────────\n",
    "\n",
    "import os\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from glob import glob\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up wandb environment variables to avoid warnings\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"TinyLlama_Headline_Generation.ipynb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"news2headline-tinyllama\"\n",
    "\n",
    "# Get wandb token and set as environment variable (PREFERRED METHOD)\n",
    "wandb_token = os.getenv(\"WANDB_KEY\")\n",
    "if wandb_token:\n",
    "    os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
    "    print(\"✅ Wandb API key set via environment variable\")\n",
    "else:\n",
    "    print(\"⚠️  Warning: WANDB_KEY not found in .env file\")\n",
    "    print(\"   Add this line to your .env file: WANDB_KEY=your_api_key_here\")\n",
    "    print(\"   Get your API key from: https://wandb.ai/authorize\")\n",
    "\n",
    "# Initialize wandb (this will automatically use WANDB_API_KEY if set)\n",
    "wandb.init(\n",
    "    project=\"news2headline-tinyllama\",\n",
    "    name=\"tinyllama-1.1b-headline-generation\",\n",
    "    notes=\"Fine-tuning TinyLlama 1.1B for news headline generation using Unsloth\",\n",
    "    tags=[\"tinyllama\", \"headline-generation\", \"unsloth\", \"lora\"],\n",
    "    config={\n",
    "        \"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "        \"max_seq_length\": max_seq_length,\n",
    "        \"r\": 32,                # Higher rank for more capacity (was 16)\n",
    "        \"lora_alpha\": 64,       # 2x rank for better learning (was 16)\n",
    "        \"lora_dropout\": 0.05,   # Small dropout for regularization (was 0)\n",
    "        \"bias\": \"lora_only\",    # Train bias terms for better adaptation\n",
    "        \"target_modules\": [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "            \"embed_tokens\", \"lm_head\"  # Include embedding layers\n",
    "        ],\n",
    "        \"use_rslora\": True,     # Rank-stabilized LoRA for better training\n",
    "        \"lora_alpha\": 16,\n",
    "        \"learning_rate\": 2e-4,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"num_train_epochs\": 1,\n",
    "        \"task\": \"headline_generation\",\n",
    "        \"dataset_size\": len(datasets[\"train\"]),\n",
    "        \"validation_size\": len(datasets[\"validation\"]),\n",
    "        \"test_size\": len(datasets[\"test\"])\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"🚀 Wandb run initialized: {wandb.run.name}\")\n",
    "print(f\"📊 Dashboard: {wandb.run.url}\")\n",
    "\n",
    "# Check for existing checkpoints\n",
    "output_dir = \"outputs\"\n",
    "checkpoint_dirs = glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "resume_from_checkpoint = None\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    # Sort by checkpoint number and get the latest one\n",
    "    checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "    resume_from_checkpoint = checkpoint_dirs[-1]\n",
    "    print(f\"Found checkpoint: {resume_from_checkpoint}\")\n",
    "    print(\"Resuming training from checkpoint...\")\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from beginning...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = datasets[\"train\"],\n",
    "    eval_dataset = datasets[\"validation\"],\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 8,\n",
    "        warmup_steps = 5,\n",
    "        warmup_ratio = 0.1,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        num_train_epochs = 2, # Set this for 1 full training run.\n",
    "        max_steps = -1,\n",
    "        learning_rate = 1e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.05,   # Higher weight decay to prevent overfitting\n",
    "        max_grad_norm = 0.5,   # Gradient clipping for stability\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = output_dir,\n",
    "        report_to = \"wandb\", # Enable wandb logging\n",
    "        remove_unused_columns = False,  # Add this line\n",
    "        save_strategy = \"steps\",\n",
    "        save_steps = 250,  # Save checkpoint every 500 steps\n",
    "        eval_strategy = \"steps\",\n",
    "        eval_steps = 250,  # Evaluate every 500 steps\n",
    "        logging_steps = 50,\n",
    "        load_best_model_at_end = True,\n",
    "        logging_strategy = \"steps\",\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        greater_is_better = False,\n",
    "        \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4eff449bb196e7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:37.552526Z",
     "start_time": "2025-07-19T14:34:35.269930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20953faa9c14526b36010b1c2a4e60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/41681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb36ae5d2f543cf9434448fa47fc0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=20):   0%|          | 0/5152 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ─── 5. APPLY TRAIN ON RESPONSES ONLY ──────────────────────────────────────────\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8552e81babe2d058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:38.506474Z",
     "start_time": "2025-07-19T14:34:38.500988Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nVeet, in celebration of its 20th anniversary in Bangladesh, is launching a special television series titled “Agiye Jao Attobisshashey” with Deepto TV, to honor the strength, resilience, and courage of the women in Bangladesh. The show will premiere on Deepto TV on November 1, 2024, and will air every Friday and Saturday at 9:20 pm. The program will be re-telecasted on Saturday and Sunday at 4.50pm. The popular Bangladeshi actress and model, Zakia Bari Mamo, will host the show. Veet’s anniversary marks two decades of helping women feel beautiful and confident in Bangladesh. The title “Agiye Jao Attobisshashey”, which means \"Move Forward with Confidence,\" reflects Veet’s commitment to inspiring women and giving them a platform to share their powerful life stories. The series will feature ten inspiring women from different backgrounds and professions who have made a turn-around in her life in spite of lot of obstacles. These women have overcome challenges, broken stereotypes, and dedicated themselves and eventually came where they are today. From business leaders to teachers, athletes to advocates, these women reflect the values of strength and determination that Veet has always supported in 20 years of its journey in Bangladesh. Bd-pratidin English/Lutful Hoque<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nVeet celebrates 20 Years of journey with \"Agiye Jao Attobisshashey” campaign<|eot_id|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3610a95286d01405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T14:34:40.502285Z",
     "start_time": "2025-07-19T14:34:40.497178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                                                                                                                                                                                                                                                                                                                             Veet celebrates 20 Years of journey with \"Agiye Jao Attobisshashey” campaign<|eot_id|>'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[5][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b62b8551761f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdf74f22f97c503f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T18:50:42.656748Z",
     "start_time": "2025-07-19T14:34:43.212508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 41,681 | Num Epochs = 1 | Total steps = 5,211\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 11,272,192 of 1,247,086,592 (0.90% trained)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5211' max='5211' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5211/5211 4:15:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.251500</td>\n",
       "      <td>1.487769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.088000</td>\n",
       "      <td>1.439021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.450800</td>\n",
       "      <td>1.414740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>1.388201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.282100</td>\n",
       "      <td>1.369782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.287100</td>\n",
       "      <td>1.350047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.173300</td>\n",
       "      <td>1.331723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.929800</td>\n",
       "      <td>1.321058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.511700</td>\n",
       "      <td>1.310141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.233900</td>\n",
       "      <td>1.302940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Final loss: 1.3965\n",
      "Total training steps: 5211\n"
     ]
    }
   ],
   "source": [
    "# ─── 6. START TRAINING WITH CHECKPOINT RESUMING ───────────────────────────────\n",
    "\n",
    "# Start training - will automatically resume from checkpoint if available\n",
    "# According to Unsloth docs, use resume_from_checkpoint=True for automatic detection\n",
    "trainer_stats = trainer.train(resume_from_checkpoint=True if resume_from_checkpoint else None)\n",
    "\n",
    "# Log final metrics to wandb\n",
    "wandb.log({\n",
    "    \"final_train_loss\": trainer_stats.training_loss,\n",
    "    \"total_steps\": trainer_stats.global_step,\n",
    "})\n",
    "\n",
    "print(f\"Training completed! Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "print(f\"Total training steps: {trainer_stats.global_step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb17c47196ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation_section",
   "metadata": {},
   "source": [
    "### Model Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "evaluation_metrics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:11:03.195457Z",
     "start_time": "2025-07-19T19:00:03.205470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive evaluation on test set...\n",
      "Test set size: 5204\n",
      "\n",
      "=== Validation Set Evaluation ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1288' max='1288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1288/1288 10:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3029\n",
      "Validation Perplexity: 3.6801\n"
     ]
    }
   ],
   "source": [
    "# ─── 7. COMPREHENSIVE MODEL EVALUATION ─────────────────────────────────────────\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "print(\"Starting comprehensive evaluation on test set...\")\n",
    "print(f\"Test set size: {len(datasets['test'])}\")\n",
    "\n",
    "# Evaluate on validation set first (faster)\n",
    "print(\"\\n=== Validation Set Evaluation ===\")\n",
    "# val_metrics = trainer.evaluate(eval_dataset=datasets[\"validation\"])\n",
    "val_metrics = trainer.evaluate()\n",
    "print(f\"Validation Loss: {val_metrics['eval_loss']:.4f}\")\n",
    "print(f\"Validation Perplexity: {np.exp(val_metrics['eval_loss']):.4f}\")\n",
    "\n",
    "# Log validation metrics to wandb\n",
    "wandb.log({\n",
    "    \"val_loss\": val_metrics['eval_loss'],\n",
    "    \"val_perplexity\": np.exp(val_metrics['eval_loss'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "generation_evaluation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:20:15.600096Z",
     "start_time": "2025-07-19T19:19:54.653864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating headlines for 100 test samples ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "  2%|▏         | 2/100 [00:00<00:26,  3.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 1 ---\n",
      "Content: Awami League General Secretary Obaidul Quader today strongly condemned the statements made by BNP Secretary General Mirza Fakhrul Islam Alamgir, calling them politically motivated, misleading, and pro...\n",
      "Reference: Conspiracy is BNP's only strategy to seize power: Quader\n",
      "Generated: Quader slams Fakhrul’s statements as ‘politically motivated’\n",
      "\n",
      "--- Example 2 ---\n",
      "Content: The Election Commission (EC) has received the go ahead to access the Rohingya database maintained by the United Nations High Commissioner for Refugees (UNHCR). The database contains records of over on...\n",
      "Reference: EC gets go ahead to access Rohingya database\n",
      "Generated: EC gets go-ahead to access Rohingya database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:00<00:23,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Example 3 ---\n",
      "Content: The Office of the Chief Prosecutor of the International Crimes Tribunal (ICT) has issued a strong condemnation of a Prothom Alo report titled “Question on Tajul Islam’s involvement in ATM Azharul’s ca...\n",
      "Reference: Prothom Alo report part of conspiracy against ICT trial proceedings\n",
      "Generated: Chief Prosecutor condemns Prothom Alo report on ICT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:20<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully generated 100 headlines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── 8. QUALITATIVE EVALUATION WITH GENERATION METRICS ────────────────────────\n",
    "\n",
    "# Prepare model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_headline(content, max_length=50):\n",
    "    \"\"\"Generate headline for given content\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test on a subset of test data for detailed metrics (first 100 samples for speed)\n",
    "test_subset = datasets[\"test\"].select(range(min(100, len(datasets[\"test\"]))))\n",
    "print(f\"\\n=== Generating headlines for {len(test_subset)} test samples ===\")\n",
    "\n",
    "generated_headlines = []\n",
    "reference_headlines = []\n",
    "\n",
    "for i, example in enumerate(tqdm(test_subset)):\n",
    "    try:\n",
    "        generated = generate_headline(example[\"content\"])\n",
    "        generated_headlines.append(generated)\n",
    "        reference_headlines.append(example[\"headline\"])\n",
    "        \n",
    "        # Show first few examples\n",
    "        if i < 3:\n",
    "            print(f\"\\n--- Example {i+1} ---\")\n",
    "            print(f\"Content: {example['content'][:200]}...\")\n",
    "            print(f\"Reference: {example['headline']}\")\n",
    "            print(f\"Generated: {generated}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating headline for example {i}: {e}\")\n",
    "        generated_headlines.append(\"\")\n",
    "        reference_headlines.append(example[\"headline\"])\n",
    "\n",
    "print(f\"\\nSuccessfully generated {len([h for h in generated_headlines if h])} headlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "comprehensive_metrics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:20:25.837746Z",
     "start_time": "2025-07-19T19:20:24.168368Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Calculating Evaluation Metrics ===\n",
      "ROUGE-1: 0.4594\n",
      "ROUGE-2: 0.2237\n",
      "ROUGE-L: 0.4122\n",
      "BLEU: 0.1033\n",
      "Avg Generated Length: 8.16 words\n",
      "Avg Reference Length: 8.57 words\n",
      "Length Ratio: 0.95\n",
      "Generation Success Rate: 100.00%\n",
      "\n",
      "=== Final Evaluation Summary ===\n",
      "Model Performance on News Headline Generation:\n",
      "• ROUGE-1 (Unigram Overlap): 0.4594\n",
      "• ROUGE-2 (Bigram Overlap): 0.2237\n",
      "• ROUGE-L (Longest Common Subsequence): 0.4122\n",
      "• BLEU Score: 0.1033\n",
      "• Generation Success Rate: 100.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▄▄▃▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>█▆▂▆▂▄▃▁▁▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▇▃▇▅▆█▇██</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▇▃▇▅▆█▇██</td></tr><tr><td>final_train_loss</td><td>▁</td></tr><tr><td>test_avg_gen_length</td><td>▁</td></tr><tr><td>test_avg_ref_length</td><td>▁</td></tr><tr><td>test_bleu</td><td>▁</td></tr><tr><td>test_length_ratio</td><td>▁</td></tr><tr><td>test_rouge1</td><td>▁</td></tr><tr><td>test_rouge2</td><td>▁</td></tr><tr><td>test_rougeL</td><td>▁</td></tr><tr><td>test_success_rate</td><td>▁</td></tr><tr><td>total_steps</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▅▂▃▂▂▃▄▃█▂▂▃▂▁▃▅▄▅▄▆▂▄▃▅▃▄▂▃▂▂▁▄▄▅▂▃▂▅▂▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>train/loss</td><td>▅▃▄▃█▆▁▅▄▅▇▃▆▁▃▄▂▃█▃█▆▁▄▄▆▃▂▂▃▄▄▆▅▃▄▃▃▂▄</td></tr><tr><td>val_loss</td><td>▁</td></tr><tr><td>val_perplexity</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.30294</td></tr><tr><td>eval/runtime</td><td>659.9435</td></tr><tr><td>eval/samples_per_second</td><td>7.807</td></tr><tr><td>eval/steps_per_second</td><td>1.952</td></tr><tr><td>final_train_loss</td><td>1.39646</td></tr><tr><td>test_avg_gen_length</td><td>8.16</td></tr><tr><td>test_avg_ref_length</td><td>8.57</td></tr><tr><td>test_bleu</td><td>0.10327</td></tr><tr><td>test_length_ratio</td><td>0.95216</td></tr><tr><td>test_rouge1</td><td>0.45936</td></tr><tr><td>test_rouge2</td><td>0.22373</td></tr><tr><td>test_rougeL</td><td>0.41219</td></tr><tr><td>test_success_rate</td><td>1</td></tr><tr><td>total_flos</td><td>1.3733713869493862e+17</td></tr><tr><td>total_steps</td><td>5211</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>5211</td></tr><tr><td>train/grad_norm</td><td>6.44781</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3738</td></tr><tr><td>train_loss</td><td>1.39646</td></tr><tr><td>train_runtime</td><td>15357.8788</td></tr><tr><td>train_samples_per_second</td><td>2.714</td></tr><tr><td>train_steps_per_second</td><td>0.339</td></tr><tr><td>val_loss</td><td>1.30294</td></tr><tr><td>val_perplexity</td><td>3.6801</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">tinyllama-1.1b-headline-generation</strong> at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama/runs/lk5p5ypv</a><br> View project at: <a href='https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama' target=\"_blank\">https://wandb.ai/aonyendopaul-american-international-university-bangladesh/news2headline-tinyllama</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250719_203429-lk5p5ypv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation completed! Check your wandb dashboard for detailed metrics.\n"
     ]
    }
   ],
   "source": [
    "# ─── 9. CALCULATE COMPREHENSIVE METRICS ────────────────────────────────────────\n",
    "\n",
    "def calculate_rouge_scores(generated, references):\n",
    "    \"\"\"Calculate ROUGE scores\"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for gen, ref in zip(generated, references):\n",
    "        if gen:  # Only calculate if generation is not empty\n",
    "            scores = scorer.score(ref, gen)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']) if rouge_scores['rouge1'] else 0,\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']) if rouge_scores['rouge2'] else 0,\n",
    "        'rougeL': np.mean(rouge_scores['rougeL']) if rouge_scores['rougeL'] else 0\n",
    "    }\n",
    "\n",
    "def calculate_bleu_scores(generated, references):\n",
    "    \"\"\"Calculate BLEU scores\"\"\"\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for gen, ref in zip(generated, references):\n",
    "        if gen:  # Only calculate if generation is not empty\n",
    "            ref_tokens = [ref.split()]\n",
    "            gen_tokens = gen.split()\n",
    "            try:\n",
    "                bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothing)\n",
    "                bleu_scores.append(bleu)\n",
    "            except:\n",
    "                bleu_scores.append(0.0)\n",
    "    \n",
    "    return np.mean(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "def calculate_length_metrics(generated, references):\n",
    "    \"\"\"Calculate length-based metrics\"\"\"\n",
    "    gen_lengths = [len(h.split()) for h in generated if h]\n",
    "    ref_lengths = [len(h.split()) for h in references]\n",
    "    \n",
    "    return {\n",
    "        'avg_generated_length': np.mean(gen_lengths) if gen_lengths else 0,\n",
    "        'avg_reference_length': np.mean(ref_lengths),\n",
    "        'length_ratio': np.mean(gen_lengths) / np.mean(ref_lengths) if gen_lengths and ref_lengths else 0\n",
    "    }\n",
    "\n",
    "# Calculate all metrics\n",
    "print(\"\\n=== Calculating Evaluation Metrics ===\")\n",
    "\n",
    "# ROUGE scores\n",
    "rouge_scores = calculate_rouge_scores(generated_headlines, reference_headlines)\n",
    "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "# BLEU score\n",
    "bleu_score = calculate_bleu_scores(generated_headlines, reference_headlines)\n",
    "print(f\"BLEU: {bleu_score:.4f}\")\n",
    "\n",
    "# Length metrics\n",
    "length_metrics = calculate_length_metrics(generated_headlines, reference_headlines)\n",
    "print(f\"Avg Generated Length: {length_metrics['avg_generated_length']:.2f} words\")\n",
    "print(f\"Avg Reference Length: {length_metrics['avg_reference_length']:.2f} words\")\n",
    "print(f\"Length Ratio: {length_metrics['length_ratio']:.2f}\")\n",
    "\n",
    "# Success rate\n",
    "success_rate = len([h for h in generated_headlines if h]) / len(generated_headlines)\n",
    "print(f\"Generation Success Rate: {success_rate:.2%}\")\n",
    "\n",
    "# Compile final metrics\n",
    "final_metrics = {\n",
    "    \"test_rouge1\": rouge_scores['rouge1'],\n",
    "    \"test_rouge2\": rouge_scores['rouge2'],\n",
    "    \"test_rougeL\": rouge_scores['rougeL'],\n",
    "    \"test_bleu\": bleu_score,\n",
    "    \"test_success_rate\": success_rate,\n",
    "    \"test_avg_gen_length\": length_metrics['avg_generated_length'],\n",
    "    \"test_avg_ref_length\": length_metrics['avg_reference_length'],\n",
    "    \"test_length_ratio\": length_metrics['length_ratio']\n",
    "}\n",
    "\n",
    "# Log to wandb\n",
    "wandb.log(final_metrics)\n",
    "\n",
    "print(\"\\n=== Final Evaluation Summary ===\")\n",
    "print(f\"Model Performance on News Headline Generation:\")\n",
    "print(f\"• ROUGE-1 (Unigram Overlap): {rouge_scores['rouge1']:.4f}\")\n",
    "print(f\"• ROUGE-2 (Bigram Overlap): {rouge_scores['rouge2']:.4f}\")\n",
    "print(f\"• ROUGE-L (Longest Common Subsequence): {rouge_scores['rougeL']:.4f}\")\n",
    "print(f\"• BLEU Score: {bleu_score:.4f}\")\n",
    "print(f\"• Generation Success Rate: {success_rate:.2%}\")\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\nEvaluation completed! Check your wandb dashboard for detailed metrics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_evaluation_section",
   "metadata": {},
   "source": [
    "# ─── 9. UNSLOTH OPTIMIZED INFERENCE & COMPREHENSIVE EVALUATION ────────────────────────────────────────"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "unsloth_inference_evaluation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T19:55:16.219480Z",
     "start_time": "2025-07-19T19:51:46.874148Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Optimizing model for inference...\n",
      "✅ Model optimized for inference (2x speed boost)\n",
      "📊 Randomly sampled 1000 examples from 5204 total test samples\n",
      "\n",
      "🔄 Running comprehensive evaluation with Unsloth optimized inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  10%|█         | 100/1000 [00:20<03:27,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 100/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  20%|██        | 200/1000 [00:41<02:43,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 200/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  30%|███       | 301/1000 [01:02<02:12,  5.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 300/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  40%|████      | 401/1000 [01:22<02:04,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 400/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  50%|█████     | 500/1000 [01:43<01:42,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 500/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  60%|██████    | 601/1000 [02:03<01:17,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 600/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  70%|███████   | 700/1000 [02:23<00:54,  5.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 700/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  80%|████████  | 801/1000 [02:44<00:38,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 800/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines:  90%|█████████ | 900/1000 [03:06<00:24,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 900/1000 | Success rate: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating headlines: 100%|██████████| 1000/1000 [03:29<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Progress: 1000/1000 | Success rate: 100.0%\n",
      "\n",
      "✅ Generation completed! Failed: 0/1000\n",
      "📊 Calculating comprehensive metrics...\n",
      "\n",
      "============================================================\n",
      "🎯 UNSLOTH OPTIMIZED INFERENCE EVALUATION REPORT\n",
      "============================================================\n",
      "\n",
      "📊 SAMPLE STATISTICS:\n",
      "   Total samples: 1000\n",
      "   Valid generations: 1000\n",
      "   Success rate: 100.00%\n",
      "\n",
      "📝 CONTENT OVERLAP METRICS:\n",
      "   ROUGE-1 (unigram): 0.4216\n",
      "   ROUGE-2 (bigram):  0.1845\n",
      "   ROUGE-L (LCS):     0.3737\n",
      "   BLEU score:        0.0860\n",
      "\n",
      "📏 LENGTH METRICS:\n",
      "   Avg generated length: 8.0 words\n",
      "   Avg reference length: 8.4 words\n",
      "   Length ratio:         0.95\n",
      "\n",
      "🎯 QUALITY ASSESSMENT:\n",
      "   Exact match rate:     1.10%\n",
      "   ✅ Good content overlap (ROUGE-1 > 0.4)\n",
      "   ✅ Good length matching\n",
      "   ✅ Excellent generation reliability\n",
      "\n",
      "📰 SAMPLE GENERATED HEADLINES:\n",
      "\n",
      "   Example 1:\n",
      "   Reference: Stock market trading sees upward trend\n",
      "   Generated: Stock market sees uptick in morning trade\n",
      "\n",
      "   Example 2:\n",
      "   Reference: Rain expected in 3 divisions\n",
      "   Generated: Thunderstorms may occur in several regions; otherwise, partly cloudy skies expected\n",
      "\n",
      "   Example 3:\n",
      "   Reference: 5 killed in separate road, train accidents\n",
      "   Generated: Road accidents kill three in Bangladesh\n",
      "\n",
      "   Example 4:\n",
      "   Reference: US and Iran close to nuclear deal: Trump\n",
      "   Generated: Trump says Iran'sort of' agreed to nuclear deal\n",
      "\n",
      "   Example 5:\n",
      "   Reference: ‘Introduce ration for RMG workers’\n",
      "   Generated: Rationing for RMG workers: Participants demand fair budget\n",
      "\n",
      "============================================================\n",
      "\n",
      "💾 Detailed results saved to: unsloth_evaluation_results.json\n",
      "✅ Unsloth optimized evaluation completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ─── UNSLOTH OPTIMIZED INFERENCE & COMPREHENSIVE EVALUATION ────────────────────\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ─── 1. OPTIMIZE MODEL FOR INFERENCE ───────────────────────────────────────────\n",
    "print(\"🚀 Optimizing model for inference...\")\n",
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "print(\"✅ Model optimized for inference (2x speed boost)\")\n",
    "\n",
    "# ─── 2. PREPARE TEST DATASET ───────────────────────────────────────────────────\n",
    "test_dataset = datasets[\"test\"]\n",
    "sample_size = min(1000, len(test_dataset))  # Use up to 1000 samples for comprehensive evaluation\n",
    "\n",
    "# Randomly sample for unbiased evaluation\n",
    "random.seed(42)\n",
    "test_indices = random.sample(range(len(test_dataset)), sample_size)\n",
    "test_samples = [test_dataset[i] for i in test_indices]\n",
    "\n",
    "print(f\"📊 Randomly sampled {sample_size} examples from {len(test_dataset)} total test samples\")\n",
    "\n",
    "# ─── 3. INFERENCE FUNCTION WITH UNSLOTH OPTIMIZATION ──────────────────────────\n",
    "def generate_headline_unsloth(content, max_new_tokens=20):\n",
    "    \"\"\"Generate headline using Unsloth optimized inference\"\"\"\n",
    "    # Create conversation format\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    # Generate with optimized settings\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            repetition_penalty=1.1,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True  # Unsloth optimization\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated part\n",
    "    generated_tokens = outputs[0][len(inputs[0]):]\n",
    "    headline = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    return headline\n",
    "\n",
    "# ─── 4. COMPREHENSIVE EVALUATION METRICS ──────────────────────────────────────\n",
    "def calculate_comprehensive_metrics(generated_headlines, reference_headlines):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    # Filter out empty generations\n",
    "    valid_pairs = [(g, r) for g, r in zip(generated_headlines, reference_headlines) if g and g.strip()]\n",
    "    \n",
    "    if not valid_pairs:\n",
    "        return {\"error\": \"No valid generations found\"}\n",
    "    \n",
    "    valid_generated = [pair[0] for pair in valid_pairs]\n",
    "    valid_references = [pair[1] for pair in valid_pairs]\n",
    "    \n",
    "    # ROUGE Scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    for gen, ref in valid_pairs:\n",
    "        scores = scorer.score(ref, gen)\n",
    "        rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "        rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "        rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # BLEU Scores\n",
    "    smoothing = SmoothingFunction().method1\n",
    "    bleu_scores = []\n",
    "    \n",
    "    for gen, ref in valid_pairs:\n",
    "        ref_tokens = [ref.split()]\n",
    "        gen_tokens = gen.split()\n",
    "        try:\n",
    "            bleu = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothing)\n",
    "            bleu_scores.append(bleu)\n",
    "        except:\n",
    "            bleu_scores.append(0.0)\n",
    "    \n",
    "    # Length Metrics\n",
    "    gen_lengths = [len(g.split()) for g in valid_generated]\n",
    "    ref_lengths = [len(r.split()) for r in valid_references]\n",
    "    \n",
    "    # Exact Match\n",
    "    exact_matches = sum(1 for g, r in valid_pairs if g.lower().strip() == r.lower().strip())\n",
    "    \n",
    "    return {\n",
    "        'rouge1': np.mean(rouge_scores['rouge1']),\n",
    "        'rouge2': np.mean(rouge_scores['rouge2']),\n",
    "        'rougeL': np.mean(rouge_scores['rougeL']),\n",
    "        'bleu': np.mean(bleu_scores),\n",
    "        'avg_generated_length': np.mean(gen_lengths),\n",
    "        'avg_reference_length': np.mean(ref_lengths),\n",
    "        'length_ratio': np.mean(gen_lengths) / np.mean(ref_lengths),\n",
    "        'success_rate': len(valid_pairs) / len(generated_headlines),\n",
    "        'exact_match_rate': exact_matches / len(valid_pairs),\n",
    "        'total_samples': len(generated_headlines),\n",
    "        'valid_generations': len(valid_pairs)\n",
    "    }\n",
    "\n",
    "# ─── 5. RUN COMPREHENSIVE EVALUATION ───────────────────────────────────────────\n",
    "print(\"\\n🔄 Running comprehensive evaluation with Unsloth optimized inference...\")\n",
    "\n",
    "generated_headlines = []\n",
    "reference_headlines = []\n",
    "failed_generations = 0\n",
    "\n",
    "# Generate headlines with progress tracking\n",
    "for i, sample in enumerate(tqdm(test_samples, desc=\"Generating headlines\")):\n",
    "    try:\n",
    "        # Generate headline\n",
    "        generated = generate_headline_unsloth(sample[\"content\"])\n",
    "        generated_headlines.append(generated)\n",
    "        reference_headlines.append(sample[\"headline\"])\n",
    "        \n",
    "        # Progress update every 100 samples\n",
    "        if (i + 1) % 100 == 0:\n",
    "            success_rate = (len(generated_headlines) - failed_generations) / len(generated_headlines) * 100\n",
    "            print(f\"   Progress: {i+1}/{sample_size} | Success rate: {success_rate:.1f}%\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   Failed to generate headline for sample {i}: {str(e)}\")\n",
    "        generated_headlines.append(\"\")\n",
    "        reference_headlines.append(sample[\"headline\"])\n",
    "        failed_generations += 1\n",
    "\n",
    "print(f\"\\n✅ Generation completed! Failed: {failed_generations}/{sample_size}\")\n",
    "\n",
    "# ─── 6. CALCULATE METRICS ──────────────────────────────────────────────────────\n",
    "print(\"📊 Calculating comprehensive metrics...\")\n",
    "metrics = calculate_comprehensive_metrics(generated_headlines, reference_headlines)\n",
    "\n",
    "# ─── 7. DISPLAY RESULTS ────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 UNSLOTH OPTIMIZED INFERENCE EVALUATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n📊 SAMPLE STATISTICS:\")\n",
    "print(f\"   Total samples: {metrics['total_samples']}\")\n",
    "print(f\"   Valid generations: {metrics['valid_generations']}\")\n",
    "print(f\"   Success rate: {metrics['success_rate']:.2%}\")\n",
    "\n",
    "print(f\"\\n📝 CONTENT OVERLAP METRICS:\")\n",
    "print(f\"   ROUGE-1 (unigram): {metrics['rouge1']:.4f}\")\n",
    "print(f\"   ROUGE-2 (bigram):  {metrics['rouge2']:.4f}\")\n",
    "print(f\"   ROUGE-L (LCS):     {metrics['rougeL']:.4f}\")\n",
    "print(f\"   BLEU score:        {metrics['bleu']:.4f}\")\n",
    "\n",
    "print(f\"\\n📏 LENGTH METRICS:\")\n",
    "print(f\"   Avg generated length: {metrics['avg_generated_length']:.1f} words\")\n",
    "print(f\"   Avg reference length: {metrics['avg_reference_length']:.1f} words\")\n",
    "print(f\"   Length ratio:         {metrics['length_ratio']:.2f}\")\n",
    "\n",
    "print(f\"\\n🎯 QUALITY ASSESSMENT:\")\n",
    "print(f\"   Exact match rate:     {metrics['exact_match_rate']:.2%}\")\n",
    "if metrics['rouge1'] > 0.4:\n",
    "    print(f\"   ✅ Good content overlap (ROUGE-1 > 0.4)\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Moderate content overlap (ROUGE-1 = {metrics['rouge1']:.4f})\")\n",
    "\n",
    "if 0.8 <= metrics['length_ratio'] <= 1.2:\n",
    "    print(f\"   ✅ Good length matching\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Length mismatch (ratio = {metrics['length_ratio']:.2f})\")\n",
    "\n",
    "if metrics['success_rate'] > 0.95:\n",
    "    print(f\"   ✅ Excellent generation reliability\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Generation reliability needs improvement\")\n",
    "\n",
    "# ─── 8. SHOW SAMPLE RESULTS ────────────────────────────────────────────────────\n",
    "print(f\"\\n📰 SAMPLE GENERATED HEADLINES:\")\n",
    "print()\n",
    "for i in range(min(5, len(generated_headlines))):\n",
    "    if generated_headlines[i]:  # Only show successful generations\n",
    "        print(f\"   Example {i+1}:\")\n",
    "        print(f\"   Reference: {reference_headlines[i]}\")\n",
    "        print(f\"   Generated: {generated_headlines[i]}\")\n",
    "        print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ─── 9. SAVE RESULTS ───────────────────────────────────────────────────────────\n",
    "results = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"evaluation_type\": \"unsloth_optimized_inference\",\n",
    "    \"sample_size\": sample_size,\n",
    "    \"metrics\": metrics,\n",
    "    \"sample_results\": [\n",
    "        {\n",
    "            \"reference\": ref,\n",
    "            \"generated\": gen,\n",
    "            \"content_preview\": test_samples[i][\"content\"][:100] + \"...\"\n",
    "        }\n",
    "        for i, (ref, gen) in enumerate(zip(reference_headlines[:10], generated_headlines[:10]))\n",
    "        if gen  # Only include successful generations\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open(\"unsloth_evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Detailed results saved to: unsloth_evaluation_results.json\")\n",
    "print(f\"✅ Unsloth optimized evaluation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wandb_logging_fix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── 10. WANDB LOGGING (FIXED) ─────────────────────────────────────────────────\n",
    "\n",
    "# Initialize wandb if not already active\n",
    "if not wandb.run:\n",
    "    wandb.init(\n",
    "        project=\"news2headline-tinyllama\",\n",
    "        name=\"unsloth-optimized-evaluation\",\n",
    "        notes=\"Comprehensive evaluation using Unsloth optimized inference\",\n",
    "        tags=[\"evaluation\", \"unsloth\", \"inference\", \"comprehensive\"]\n",
    "    )\n",
    "\n",
    "# Log comprehensive metrics\n",
    "wandb.log({\n",
    "    \"unsloth_inference/rouge1\": metrics[\"rouge1\"],\n",
    "    \"unsloth_inference/rouge2\": metrics[\"rouge2\"],\n",
    "    \"unsloth_inference/rougeL\": metrics[\"rougeL\"],\n",
    "    \"unsloth_inference/bleu\": metrics[\"bleu\"],\n",
    "    \"unsloth_inference/success_rate\": metrics[\"success_rate\"],\n",
    "    \"unsloth_inference/exact_match_rate\": metrics[\"exact_match_rate\"],\n",
    "    \"unsloth_inference/avg_gen_length\": metrics[\"avg_generated_length\"],\n",
    "    \"unsloth_inference/avg_ref_length\": metrics[\"avg_reference_length\"],\n",
    "    \"unsloth_inference/length_ratio\": metrics[\"length_ratio\"],\n",
    "    \"unsloth_inference/sample_size\": metrics[\"total_samples\"],\n",
    "    \"unsloth_inference/valid_generations\": metrics[\"valid_generations\"]\n",
    "})\n",
    "\n",
    "print(f\"📊 Metrics logged to wandb: {wandb.run.url}\")\n",
    "\n",
    "# Create a summary table for wandb\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = {\n",
    "    \"Metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\", \"Success Rate\", \"Exact Match\"],\n",
    "    \"Score\": [\n",
    "        f\"{metrics['rouge1']:.4f}\",\n",
    "        f\"{metrics['rouge2']:.4f}\",\n",
    "        f\"{metrics['rougeL']:.4f}\",\n",
    "        f\"{metrics['bleu']:.4f}\",\n",
    "        f\"{metrics['success_rate']:.2%}\",\n",
    "        f\"{metrics['exact_match_rate']:.2%}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "wandb.log({\"evaluation_summary\": wandb.Table(dataframe=summary_df)})\n",
    "\n",
    "# Log sample results\n",
    "sample_data = []\n",
    "for i in range(min(10, len(generated_headlines))):\n",
    "    if generated_headlines[i]:\n",
    "        sample_data.append([\n",
    "            reference_headlines[i],\n",
    "            generated_headlines[i],\n",
    "            test_samples[i][\"content\"][:150] + \"...\"\n",
    "        ])\n",
    "\n",
    "if sample_data:\n",
    "    sample_df = pd.DataFrame(sample_data, columns=[\"Reference\", \"Generated\", \"Content Preview\"])\n",
    "    wandb.log({\"sample_results\": wandb.Table(dataframe=sample_df)})\n",
    "\n",
    "print(\"✅ Comprehensive evaluation data logged to wandb\")\n",
    "print(f\"🔗 View results: {wandb.run.url}\")\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n",
    "print(\"📊 Wandb run completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
